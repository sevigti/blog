[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "To be started soon."
  },
  {
    "objectID": "projects.html#soon",
    "href": "projects.html#soon",
    "title": "Projects",
    "section": "",
    "text": "To be started soon."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "2.Image Classifier: Cleaning Data\n\n\n\n\n\n\nfastai_course\n\n\nML\n\n\nchapter2\n\n\n\n\n\n\n\n\n\nOct 30, 2024\n\n\nvisi\n\n\n\n\n\n\n\n\n\n\n\n\n01 Intro to Machine Learning\n\n\n\n\n\n\nfastai_course\n\n\nchapter_1\n\n\nML\n\n\n\nBuild an image recognizer\n\n\n\n\n\nOct 23, 2024\n\n\nvisi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, I’m Elvis. I’m a full-stack developer who enjoys learning new things and building web apps."
  },
  {
    "objectID": "posts/01_intro/index.html#understanding-dataloaders",
    "href": "posts/01_intro/index.html#understanding-dataloaders",
    "title": "01 Intro to Machine Learning",
    "section": "Understanding DataLoaders",
    "text": "Understanding DataLoaders\nTo train a model, we’ll need DataLoaders, which is an object that contains:\nA training set (the images used to create the model) A validation set (the images used to check the accuracy of a model)\n\nWhat goes into the DataLoaders object?\nOut of hundreds of projects, what are all the things that change from project to project to get the data in the right shape? We can split it down into these components:\n\nInput and Output Types\n\nInput: Images\nOutput: Categories (car, bicycle, electric scooter)\n\nGetting Items (get_items)\n\nGets all image files from the specified path\nRuns the get_image_files function\nReturns a list of all image files in a path\nLooks through directories recursively\n\nData Splitting (splitter)\n\nSplits the data into training and validation sets randomly\nUses 20% of the data for the validation set\n\nLabeling (get_y=parent_label)\n\nUses the parent folder name as the category label\nExample:\n\ncar_bicycle_or_scooter/car/image1.jpg → label is “car” car_bicycle_or_scooter/bicycle/image2.jpg → label is “bicycle”\nImage Preprocessing\n\nBefore training, resize each image to 192x192 pixels\nUses “squish” method (as opposed to cropping)\n‘squish’ method maintains aspect ratio\n\nDataLoader Creation\n\ndataloaders(path, bs=32)\nCreates train and validation dataloaders\nbs=32 means batch size of 32 images\n\n\n\n\nCode\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    # car_bicycle_or_scooter/car/image1.jpg → label is \"car\"\n    # car_bicycle_or_scooter/bicycle/image2.jpg → label is \"bicycle\"\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32) \n\ndls.show_batch(max_n=6) # shows 6 images from a batch, displays both images and their labels\n\n\n\n\n\nNow we’re ready to train our model. The fastest widely used computer vision model is resnet18. You can train this in a few minutes,even on a CPU! (On a GPU, it generally takes under 10 seconds…)fastai comes with a helpful fine_tune() method which automatically uses best practices for fine tuning a pre-trained model, sowe’ll use that.\n\n\nCode\n# Create a vison learner using the DataLoaders we created above, resNet18 pre-trained model, and a metrics to measure the error rate.\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.668419\n0.081710\n0.025210\n00:03\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.067997\n0.040579\n0.004202\n00:03\n\n\n1\n0.035534\n0.002521\n0.000000\n00:03\n\n\n2\n0.026271\n0.001035\n0.000000\n00:03"
  },
  {
    "objectID": "posts/01_intro/index.html#understanding-error-rate-vs-loss",
    "href": "posts/01_intro/index.html#understanding-error-rate-vs-loss",
    "title": "01 Intro to Machine Learning",
    "section": "Understanding Error Rate vs Loss",
    "text": "Understanding Error Rate vs Loss\n\nError rate is binary (right or wrong prediction)\nLoss measures the model’s confidence/uncertainty\nExample:\nPrediction 1: Car (60% confident) ✓ Correct\nPrediction 2: Car (95% confident) ✓ Still Correct\nBoth have same error rate (0% - both correct)\nBut second prediction has lower loss (more confident)\n\n\nLoss Improvement Analysis\n\nModel is getting more confident in its correct predictions\nLess uncertainty in its decisions\nBetter internal representations of features"
  },
  {
    "objectID": "posts/01_intro/index.html#training-evolution---better-internal-representation-of-features-example",
    "href": "posts/01_intro/index.html#training-evolution---better-internal-representation-of-features-example",
    "title": "01 Intro to Machine Learning",
    "section": "Training Evolution - better internal representation of features example",
    "text": "Training Evolution - better internal representation of features example\nEarly training:\n“This is a car because it has wheels” (less confident, higher loss)\nLater training:\n“This is a car because it has wheels, specific body shape, headlights, and typical car proportions” (more confident, lower loss)\nChanges in loss don’t affect actual right/wrong decisions (error rate)"
  },
  {
    "objectID": "posts/02_image_classifier_clean/index.html",
    "href": "posts/02_image_classifier_clean/index.html",
    "title": "2.Image Classifier: Cleaning Data",
    "section": "",
    "text": "Code\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\ndiscuss the capabilities and constraints of deep learning, explore how to create datasets, look at possible gotchas when using deep learning in practice, and more.\nWhen selecting a project, the most important consideration is data availability. The goal is not to find the “perfect” dataset or project, but just to get started and iterate from there.\n\nwe will expand on the car, bicycle and electric scooter image classifier from chapter 1\nusing bing serach\nDataLoaders and what they are/do\nwhy squishing or cropping not not ideal\nwhat to use instead; Instead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.\n\n\n\nCode\nfrom fastai.vision.widgets import *\n\n\n\n\nCode\n!  pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n\n\n\nCode\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\n\n\nCode\nsearch_images_ddg\n\n\n&lt;function fastbook.search_images_ddg(term, max_images=200)&gt;\n\n\n\nimage recognizer for car, bicycle and electric scooter\nDownload images of cars using ddg\n\n\nCode\nresults = search_images_ddg('car') # list of urls of cars\nims = results.attrgot('contentUrl')\nlen(ims)\n\n\n200\n\n\n\n\nCode\nresults[0]\n\n\n'https://carsmechinery.com/wp-content/uploads/2023/02/58758dd824ee488384fbcb5a94ddb514-scaled.jpg'\n\n\n\n\nCode\n ??verify_images   # documentation of a function \n\n\nObject `verify_images   # documentation of a function` not found.\n\n\n\n\nCode\nims = ['https://th.bing.com/th/id/OIP.XQ6CyncXgEdljRssh_LAIwHaEK?rs=1&pid=ImgDetMain']\n\n\n\n\nCode\ndest = 'images/car.jpg'\ndownload_url(ims[0], dest)\n\n\n\n\n\n\n\n    \n      \n      117.72% [32768/27836 00:00&lt;00:00]\n    \n    \n\n\nPath('images/car.jpg')\n\n\n\n\nCode\nim = Image.open(dest)\nim.to_thumb(128,128)\n\n\n\n\n\n\n\n\n\nUse fastai’s download_images to download all the URLs for each of our search terms. We’ll put each in a separate folder:\n\n\nCode\nsearches = 'car','bicycle','e-scooter' \npath = Path('car_bicycle_or_escooter')\n\n\n\n\nCode\nif not path.exists():\n    path.mkdir()\n    for o in searches:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_ddg(f'{o}',100)\n        download_images(dest, urls=results)\n\n\n\n\nCode\nfns = get_image_files(path)\nfns\n\n\n(#285) [Path('car_bicycle_or_escooter/car/46e219ab-a63a-4fd8-b645-f5cfae44cdf2.jpg'),Path('car_bicycle_or_escooter/car/2dcba47a-e5a5-4ad4-a487-2e59c0537e5b.jpg'),Path('car_bicycle_or_escooter/car/965167a9-1a37-4652-a35d-51cc7dfba7cd.jpg'),Path('car_bicycle_or_escooter/car/ba89ac95-7d78-4c76-afb5-754d90bfa62c.jpg'),Path('car_bicycle_or_escooter/car/f391b233-6d75-4050-9f64-9ca1a0ec3fe3.jpg'),Path('car_bicycle_or_escooter/car/14cc0f49-b9f1-4bff-b687-befea160c56e.jpg'),Path('car_bicycle_or_escooter/car/fe549b66-9f30-49bd-b6cf-ffa867f15796.jpg'),Path('car_bicycle_or_escooter/car/56b1df58-d448-43cd-9134-2e0366227e3a.jpg'),Path('car_bicycle_or_escooter/car/6eace0e7-f482-4400-91f9-93998afcac28.png'),Path('car_bicycle_or_escooter/car/b0ce8796-653f-45c1-996c-b9bcdfe79d72.jpg')...]\n\n\nFolder sctructure. 272 labelled images downloaded above.\n\nOften when we download files from the internet, there are a few that are corrupt. Let’s check:\n\n\nCode\nfailed = verify_images(fns)\nfailed\n\n\n(#18) [Path('car_bicycle_or_escooter/car/6eace0e7-f482-4400-91f9-93998afcac28.png'),Path('car_bicycle_or_escooter/car/a25fac5a-48e5-404f-9d40-2a55539878bb.jpg'),Path('car_bicycle_or_escooter/car/d34f815b-4502-4e09-bc95-27d4d78db518.jpg'),Path('car_bicycle_or_escooter/car/409cd34a-292b-4c14-a227-24c20cf8dab2.jpg'),Path('car_bicycle_or_escooter/car/453c2258-b757-46c7-bd7e-60cad26aa581.jpg'),Path('car_bicycle_or_escooter/car/86e22584-acad-4918-85a1-755aadba3f24.jpg'),Path('car_bicycle_or_escooter/bicycle/b77bfed1-5915-4a78-853d-0ee4035b42c9.jpg'),Path('car_bicycle_or_escooter/bicycle/b08a9e3b-68ee-47b1-b930-0d42763d25e9.jpg'),Path('car_bicycle_or_escooter/bicycle/66bf96d2-0fa0-4149-bfc9-f36e9414f936.jpg'),Path('car_bicycle_or_escooter/bicycle/5d6433c9-2518-428c-9880-0ce5d497dd40.svg')...]\n\n\n\n\nCode\nfailed.map(Path.unlink)\n\n\n(#18) [None,None,None,None,None,None,None,None,None,None...]\n\n\n\n\nCode\nfns = get_image_files(path)  # get new list of the files in our path\nfns  # from 272, now we have 254 files, after unlinking the corrupted files.\n\n\n(#267) [Path('car_bicycle_or_escooter/car/46e219ab-a63a-4fd8-b645-f5cfae44cdf2.jpg'),Path('car_bicycle_or_escooter/car/2dcba47a-e5a5-4ad4-a487-2e59c0537e5b.jpg'),Path('car_bicycle_or_escooter/car/965167a9-1a37-4652-a35d-51cc7dfba7cd.jpg'),Path('car_bicycle_or_escooter/car/ba89ac95-7d78-4c76-afb5-754d90bfa62c.jpg'),Path('car_bicycle_or_escooter/car/f391b233-6d75-4050-9f64-9ca1a0ec3fe3.jpg'),Path('car_bicycle_or_escooter/car/14cc0f49-b9f1-4bff-b687-befea160c56e.jpg'),Path('car_bicycle_or_escooter/car/fe549b66-9f30-49bd-b6cf-ffa867f15796.jpg'),Path('car_bicycle_or_escooter/car/56b1df58-d448-43cd-9134-2e0366227e3a.jpg'),Path('car_bicycle_or_escooter/car/b0ce8796-653f-45c1-996c-b9bcdfe79d72.jpg'),Path('car_bicycle_or_escooter/car/f6e5db6a-4632-4b34-a098-187301a0fd05.jpg')...]\n\n\n\n\nData Loaders\nA fastai class that stores multiple DataLoader objects you pass to it, normally a train and valid. The first two are available as properties. To turn our downloaded data into a DataLoaders object we need to tell fastai at least four things:\n\nWhat kind of data are we working with\nHow to get the list of items\nHow to label these items\nHow to create the validation set\n\nIndependent variable is the thing we are using to make predictions from, and the dependent variable is our target. In our case the independent variables are the images,and our dependent variables are the categories(car, bicycle, electric scooter)\nget_image_files function takes a path, and returns a list of all of the images in that path (recursively, by default):\nwe simply want to split our training and validation sets randomly. However, we would like to have the same training/validation split each time we run this notebook, so we fix the random seed (computers don’t really know how to create random numbers at all, but simply create lists of numbers that look random; if you provide the same starting point for that list each time—called the seed—then you will get the exact same list each time):\nThe independent variable is often referred to as x and the dependent variable is often referred to as y. Here, we are telling fastai what function to call to create the labels in our dataset:\nparent_label is a function provided by fastai that simply gets the name of the folder a file is in. Because we put each of our searches images into folders named after them, this is going to give us the labels that we need.\nOur images are all different sizes, and this is a problem for deep learning: we don’t feed the model one image at a time but several of them (what we call a mini-batch). To group them in a big array (usually called a tensor) that is going to go through our model, they all need to be of the same size.\nThis command has given us a DataBlock object. This is like a template for creating a DataLoaders. We still need to tell fastai the actual source of our data—in this case, the path where the images can be found:\n\n\nCode\ndata = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\n\n\n\nCode\ndls = data.dataloaders(path)\n\n\nA DataLoader includes validation and training DataLoaders. DataLoader is a class that provides batches of a few items at a time to the GPU. When you loop through a DataLoader fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the show_batch method on a DataLoader:\n\n\nCode\ndls.valid.show_batch(max_n=5, nrows=1)\n\n\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nBy default Resize crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. Alternatively, you can ask fastai to pad the images with zeros (black), or squish/stretch them\n\n\nCode\ndata = data.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = data.dataloaders(path)\ndls.valid.show_batch(max_n=5, nrows=1)\n\n\n\n\n\n\n\n\n\nAll of these approaches seem somewhat wasteful, or problematic. If we squish or stretch the images they end up as unrealistic shapes, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy.\nInstead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.\nIn fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn’t even recognize that when an object is rotated by one degree, it still is a picture of the same thing! So actually training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image.\nHere’s another example where we replace Resize with RandomResizedCrop, which is the transform that provides the behavior we just described. The most important parameter to pass in is min_scale, which determines how much of the image to select at minimum each time:\n\n\nCode\ndata = data.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = data.dataloaders(path)\ndls.train.show_batch(max_n=5, nrows=1, unique=True)\n\n\n\n\n\n\n\n\n\n\n\nData augmentation\nData augmentation refers to creating random variations of our input data, such that they appear different, but do not actually change the meaning of the data. Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes and contrast changes.\nBecause our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time. To tell fastai we want to use these transforms on a batch, we use the batch_tfms\n\n\nCode\ndata = data.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = data.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\n\n\n\n\n\n\n\nTraining Your Model, and Using It to Clean Your Data\nWe don’t have a lot of data(253 files) for our problem so to train our model, we’ll use RandomResizedCrop with an image size of 224 px, which is fairly standard for image classification, and default aug_transforms\n\n\nCode\ndata = data.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = data.dataloaders(path)\n\n\nWe can now create our Learner and fine-tune it in the usual way\n\n\nCode\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.407114\n1.622928\n0.547170\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.657876\n0.710460\n0.264151\n00:04\n\n\n1\n0.460851\n0.358860\n0.094340\n-1:59:40\n\n\n2\n0.365095\n0.166696\n0.037736\n00:03\n\n\n3\n0.291159\n0.112446\n0.037736\n00:03\n\n\n\n\n\n\n\nconfusion matrix\nIt’s helpful to see where exactly our errors are occurring, to see whether they’re due to a dataset problem (e.g., images that aren’t bears at all, or are labeled incorrectly, etc.), or a model problem (perhaps it isn’t handling images taken with unusual lighting, or from a different angle, etc.). To do this, we can sort our images by their loss. The loss is a number that is higher if the model is incorrect (especially if it’s also confident of its incorrect answer), or if it’s correct, but not confident of its correct answer.\n\n\nCode\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot_top_losses For now, plot_top_losses shows us the images with the highest loss in our dataset. As the title of the output says, each image is labeled with four things: prediction, actual (target label), loss, and probability. The probability here is the confidence level, from zero to one, that the model has assigned to its prediction:\n\n\nCode\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Cleaning\nCleaning the data and getting it ready for your model are two of the biggest challenges for data scientists; they say it takes 90% of their time. The fastai library aims to provide tools that make it as easy as possible.\n\n\nCode\n#hide_output\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\n\n\n\ncar - card brands logos\nscooter - motor bike scooter.\nbicycle - motorbikes\n\n\nCode\ncleaner.delete()  \n\n\n(#0) []\n\n\nExecute the below code after each marking, e.g marking car-train for deletion.\n\n\nCode\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\n\n\nCode\nfns = get_image_files(path)\nfns\n\n\n(#267) [Path('car_bicycle_or_escooter/car/46e219ab-a63a-4fd8-b645-f5cfae44cdf2.jpg'),Path('car_bicycle_or_escooter/car/2dcba47a-e5a5-4ad4-a487-2e59c0537e5b.jpg'),Path('car_bicycle_or_escooter/car/965167a9-1a37-4652-a35d-51cc7dfba7cd.jpg'),Path('car_bicycle_or_escooter/car/ba89ac95-7d78-4c76-afb5-754d90bfa62c.jpg'),Path('car_bicycle_or_escooter/car/f391b233-6d75-4050-9f64-9ca1a0ec3fe3.jpg'),Path('car_bicycle_or_escooter/car/14cc0f49-b9f1-4bff-b687-befea160c56e.jpg'),Path('car_bicycle_or_escooter/car/fe549b66-9f30-49bd-b6cf-ffa867f15796.jpg'),Path('car_bicycle_or_escooter/car/56b1df58-d448-43cd-9134-2e0366227e3a.jpg'),Path('car_bicycle_or_escooter/car/b0ce8796-653f-45c1-996c-b9bcdfe79d72.jpg'),Path('car_bicycle_or_escooter/car/f6e5db6a-4632-4b34-a098-187301a0fd05.jpg')...]\n\n\n\n\nOnce we’ve cleaned up our data, we can retrain our model.\nTry it yourself, and see if your accuracy improves! After data cleaning we have 229 files from 254.\n\n\nCode\ndata = data.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = data.dataloaders(path)\n\n\n\n\nCode\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.544040\n1.980612\n0.622642\n00:03\n\n\n\n\n\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.504909\n0.851301\n0.264151\n00:28\n\n\n1\n0.407720\n0.342550\n0.094340\n00:03\n\n\n2\n0.311843\n0.132086\n0.075472\n00:04\n\n\n3\n0.255726\n0.057265\n0.018868\n00:04\n\n\n\n\n\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\nNow we get much lower error rate, compared to before cleaning the data. As you can see, the common complaint that you need massive amounts of data to do deep learning can be a very long way from the truth"
  },
  {
    "objectID": "posts/02_image_classifier_clean/2.image classifier.html",
    "href": "posts/02_image_classifier_clean/2.image classifier.html",
    "title": "Goal:",
    "section": "",
    "text": "import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ndiscuss the capabilities and constraints of deep learning, explore how to create datasets, look at possible gotchas when using deep learning in practice, and more.\nWhen selecting a project, the most important consideration is data availability. The goal is not to find the “perfect” dataset or project, but just to get started and iterate from there.\n\nwe will expand on the car, bicycle and electric scooter image classifier from chapter 1\nusing bing serach\nDataLoaders and what they are/do\nwhy squishing or cropping not not ideal\nwhat to use instead; Instead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.\n\n\nfrom fastai.vision.widgets import *\n\n\n!  pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\nsearch_images_ddg\n\n&lt;function fastbook.search_images_ddg(term, max_images=200)&gt;\n\n\n\nimage recognizer for car, bicycle and electric scooter\nDownload images of cars using ddg\n\nresults = search_images_ddg('car') # list of urls of cars\nims = results.attrgot('contentUrl')\nlen(ims)\n\n200\n\n\n\nresults[0]\n\n'https://carsmechinery.com/wp-content/uploads/2023/02/58758dd824ee488384fbcb5a94ddb514-scaled.jpg'\n\n\n\n ??verify_images   # documentation of a function \n\nObject `verify_images   # documentation of a function` not found.\n\n\n\nims = ['https://th.bing.com/th/id/OIP.XQ6CyncXgEdljRssh_LAIwHaEK?rs=1&pid=ImgDetMain']\n\n\ndest = 'images/car.jpg'\ndownload_url(ims[0], dest)\n\n\n\n\n\n\n    \n      \n      117.72% [32768/27836 00:00&lt;00:00]\n    \n    \n\n\nPath('images/car.jpg')\n\n\n\nim = Image.open(dest)\nim.to_thumb(128,128)\n\n\n\n\n\n\n\n\nUse fastai’s download_images to download all the URLs for each of our search terms. We’ll put each in a separate folder:\n\nsearches = 'car','bicycle','e-scooter' \npath = Path('car_bicycle_or_escooter')\n\n\nif not path.exists():\n    path.mkdir()\n    for o in searches:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_ddg(f'{o}',100)\n        download_images(dest, urls=results)\n\n\nfns = get_image_files(path)\nfns\n\n(#285) [Path('car_bicycle_or_escooter/car/46e219ab-a63a-4fd8-b645-f5cfae44cdf2.jpg'),Path('car_bicycle_or_escooter/car/2dcba47a-e5a5-4ad4-a487-2e59c0537e5b.jpg'),Path('car_bicycle_or_escooter/car/965167a9-1a37-4652-a35d-51cc7dfba7cd.jpg'),Path('car_bicycle_or_escooter/car/ba89ac95-7d78-4c76-afb5-754d90bfa62c.jpg'),Path('car_bicycle_or_escooter/car/f391b233-6d75-4050-9f64-9ca1a0ec3fe3.jpg'),Path('car_bicycle_or_escooter/car/14cc0f49-b9f1-4bff-b687-befea160c56e.jpg'),Path('car_bicycle_or_escooter/car/fe549b66-9f30-49bd-b6cf-ffa867f15796.jpg'),Path('car_bicycle_or_escooter/car/56b1df58-d448-43cd-9134-2e0366227e3a.jpg'),Path('car_bicycle_or_escooter/car/6eace0e7-f482-4400-91f9-93998afcac28.png'),Path('car_bicycle_or_escooter/car/b0ce8796-653f-45c1-996c-b9bcdfe79d72.jpg')...]\n\n\nFolder sctructure. 272 labelled images downloaded above.\n\nOften when we download files from the internet, there are a few that are corrupt. Let’s check:\n\nfailed = verify_images(fns)\nfailed\n\n(#18) [Path('car_bicycle_or_escooter/car/6eace0e7-f482-4400-91f9-93998afcac28.png'),Path('car_bicycle_or_escooter/car/a25fac5a-48e5-404f-9d40-2a55539878bb.jpg'),Path('car_bicycle_or_escooter/car/d34f815b-4502-4e09-bc95-27d4d78db518.jpg'),Path('car_bicycle_or_escooter/car/409cd34a-292b-4c14-a227-24c20cf8dab2.jpg'),Path('car_bicycle_or_escooter/car/453c2258-b757-46c7-bd7e-60cad26aa581.jpg'),Path('car_bicycle_or_escooter/car/86e22584-acad-4918-85a1-755aadba3f24.jpg'),Path('car_bicycle_or_escooter/bicycle/b77bfed1-5915-4a78-853d-0ee4035b42c9.jpg'),Path('car_bicycle_or_escooter/bicycle/b08a9e3b-68ee-47b1-b930-0d42763d25e9.jpg'),Path('car_bicycle_or_escooter/bicycle/66bf96d2-0fa0-4149-bfc9-f36e9414f936.jpg'),Path('car_bicycle_or_escooter/bicycle/5d6433c9-2518-428c-9880-0ce5d497dd40.svg')...]\n\n\n\nfailed.map(Path.unlink)\n\n(#18) [None,None,None,None,None,None,None,None,None,None...]\n\n\n\nfns = get_image_files(path)  # get new list of the files in our path\nfns  # from 272, now we have 254 files, after unlinking the corrupted files.\n\n(#267) [Path('car_bicycle_or_escooter/car/46e219ab-a63a-4fd8-b645-f5cfae44cdf2.jpg'),Path('car_bicycle_or_escooter/car/2dcba47a-e5a5-4ad4-a487-2e59c0537e5b.jpg'),Path('car_bicycle_or_escooter/car/965167a9-1a37-4652-a35d-51cc7dfba7cd.jpg'),Path('car_bicycle_or_escooter/car/ba89ac95-7d78-4c76-afb5-754d90bfa62c.jpg'),Path('car_bicycle_or_escooter/car/f391b233-6d75-4050-9f64-9ca1a0ec3fe3.jpg'),Path('car_bicycle_or_escooter/car/14cc0f49-b9f1-4bff-b687-befea160c56e.jpg'),Path('car_bicycle_or_escooter/car/fe549b66-9f30-49bd-b6cf-ffa867f15796.jpg'),Path('car_bicycle_or_escooter/car/56b1df58-d448-43cd-9134-2e0366227e3a.jpg'),Path('car_bicycle_or_escooter/car/b0ce8796-653f-45c1-996c-b9bcdfe79d72.jpg'),Path('car_bicycle_or_escooter/car/f6e5db6a-4632-4b34-a098-187301a0fd05.jpg')...]\n\n\n\n\nData Loaders\nA fastai class that stores multiple DataLoader objects you pass to it, normally a train and valid. The first two are available as properties. To turn our downloaded data into a DataLoaders object we need to tell fastai at least four things:\n\nWhat kind of data are we working with\nHow to get the list of items\nHow to label these items\nHow to create the validation set\n\nIndependent variable is the thing we are using to make predictions from, and the dependent variable is our target. In our case the independent variables are the images,and our dependent variables are the categories(car, bicycle, electric scooter)\nget_image_files function takes a path, and returns a list of all of the images in that path (recursively, by default):\nwe simply want to split our training and validation sets randomly. However, we would like to have the same training/validation split each time we run this notebook, so we fix the random seed (computers don’t really know how to create random numbers at all, but simply create lists of numbers that look random; if you provide the same starting point for that list each time—called the seed—then you will get the exact same list each time):\nThe independent variable is often referred to as x and the dependent variable is often referred to as y. Here, we are telling fastai what function to call to create the labels in our dataset:\nparent_label is a function provided by fastai that simply gets the name of the folder a file is in. Because we put each of our searches images into folders named after them, this is going to give us the labels that we need.\nOur images are all different sizes, and this is a problem for deep learning: we don’t feed the model one image at a time but several of them (what we call a mini-batch). To group them in a big array (usually called a tensor) that is going to go through our model, they all need to be of the same size.\nThis command has given us a DataBlock object. This is like a template for creating a DataLoaders. We still need to tell fastai the actual source of our data—in this case, the path where the images can be found:\n\ndata = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\n\ndls = data.dataloaders(path)\n\nA DataLoader includes validation and training DataLoaders. DataLoader is a class that provides batches of a few items at a time to the GPU. When you loop through a DataLoader fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the show_batch method on a DataLoader:\n\ndls.valid.show_batch(max_n=5, nrows=1)\n\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nBy default Resize crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. Alternatively, you can ask fastai to pad the images with zeros (black), or squish/stretch them\n\ndata = data.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = data.dataloaders(path)\ndls.valid.show_batch(max_n=5, nrows=1)\n\n\n\n\n\n\n\n\nAll of these approaches seem somewhat wasteful, or problematic. If we squish or stretch the images they end up as unrealistic shapes, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy.\nInstead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.\nIn fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn’t even recognize that when an object is rotated by one degree, it still is a picture of the same thing! So actually training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image.\nHere’s another example where we replace Resize with RandomResizedCrop, which is the transform that provides the behavior we just described. The most important parameter to pass in is min_scale, which determines how much of the image to select at minimum each time:\n\ndata = data.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = data.dataloaders(path)\ndls.train.show_batch(max_n=5, nrows=1, unique=True)\n\n\n\n\n\n\n\n\n\n\nData augmentation\nData augmentation refers to creating random variations of our input data, such that they appear different, but do not actually change the meaning of the data. Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes and contrast changes.\nBecause our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time. To tell fastai we want to use these transforms on a batch, we use the batch_tfms\n\ndata = data.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = data.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\n\n\n\n\n\n\nTraining Your Model, and Using It to Clean Your Data\nWe don’t have a lot of data(253 files) for our problem so to train our model, we’ll use RandomResizedCrop with an image size of 224 px, which is fairly standard for image classification, and default aug_transforms\n\ndata = data.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = data.dataloaders(path)\n\nWe can now create our Learner and fine-tune it in the usual way\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.407114\n1.622928\n0.547170\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.657876\n0.710460\n0.264151\n00:04\n\n\n1\n0.460851\n0.358860\n0.094340\n-1:59:40\n\n\n2\n0.365095\n0.166696\n0.037736\n00:03\n\n\n3\n0.291159\n0.112446\n0.037736\n00:03\n\n\n\n\n\n\n\nconfusion matrix\nIt’s helpful to see where exactly our errors are occurring, to see whether they’re due to a dataset problem (e.g., images that aren’t bears at all, or are labeled incorrectly, etc.), or a model problem (perhaps it isn’t handling images taken with unusual lighting, or from a different angle, etc.). To do this, we can sort our images by their loss. The loss is a number that is higher if the model is incorrect (especially if it’s also confident of its incorrect answer), or if it’s correct, but not confident of its correct answer.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot_top_losses For now, plot_top_losses shows us the images with the highest loss in our dataset. As the title of the output says, each image is labeled with four things: prediction, actual (target label), loss, and probability. The probability here is the confidence level, from zero to one, that the model has assigned to its prediction:\n\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Cleaning\nCleaning the data and getting it ready for your model are two of the biggest challenges for data scientists; they say it takes 90% of their time. The fastai library aims to provide tools that make it as easy as possible.\n\n#hide_output\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\n\n\n\ncar - card brands logos\nscooter - motor bike scooter.\nbicycle - motorbikes\n\ncleaner.delete()  \n\n(#0) []\n\n\nExecute the below code after each marking, e.g marking car-train for deletion.\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\nfns = get_image_files(path)\nfns\n\n(#267) [Path('car_bicycle_or_escooter/car/46e219ab-a63a-4fd8-b645-f5cfae44cdf2.jpg'),Path('car_bicycle_or_escooter/car/2dcba47a-e5a5-4ad4-a487-2e59c0537e5b.jpg'),Path('car_bicycle_or_escooter/car/965167a9-1a37-4652-a35d-51cc7dfba7cd.jpg'),Path('car_bicycle_or_escooter/car/ba89ac95-7d78-4c76-afb5-754d90bfa62c.jpg'),Path('car_bicycle_or_escooter/car/f391b233-6d75-4050-9f64-9ca1a0ec3fe3.jpg'),Path('car_bicycle_or_escooter/car/14cc0f49-b9f1-4bff-b687-befea160c56e.jpg'),Path('car_bicycle_or_escooter/car/fe549b66-9f30-49bd-b6cf-ffa867f15796.jpg'),Path('car_bicycle_or_escooter/car/56b1df58-d448-43cd-9134-2e0366227e3a.jpg'),Path('car_bicycle_or_escooter/car/b0ce8796-653f-45c1-996c-b9bcdfe79d72.jpg'),Path('car_bicycle_or_escooter/car/f6e5db6a-4632-4b34-a098-187301a0fd05.jpg')...]\n\n\n\n\nOnce we’ve cleaned up our data, we can retrain our model.\nTry it yourself, and see if your accuracy improves! After data cleaning we have 229 files from 254.\n\ndata = data.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = data.dataloaders(path)\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.544040\n1.980612\n0.622642\n00:03\n\n\n\n\n\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.504909\n0.851301\n0.264151\n00:28\n\n\n1\n0.407720\n0.342550\n0.094340\n00:03\n\n\n2\n0.311843\n0.132086\n0.075472\n00:04\n\n\n3\n0.255726\n0.057265\n0.018868\n00:04\n\n\n\n\n\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/home/visi/blog/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\nNow we get much lower error rate, compared to before cleaning the data. As you can see, the common complaint that you need massive amounts of data to do deep learning can be a very long way from the truth"
  }
]